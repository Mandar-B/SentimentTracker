{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc681df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd574f",
   "metadata": {},
   "source": [
    "# Hw 5: Sentiment Analysis on Scraped News Data üì∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3eb239",
   "metadata": {},
   "source": [
    "Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "Collaborators:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359c77",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "For this homework, work through **Lab 5 (Sentiment Analysis on Movie Reviews)** first. Most of the things we ask you to do in this homework are explained in the lab. In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, and a legend if it is applicable).\n",
    "\n",
    "Frequently **save** your notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932050d1",
   "metadata": {},
   "source": [
    "### Collaborators and Sources\n",
    "Furthermore, in addition to recording your **collaborators** on this homework, please also remember to **cite/indicate all external sources** used when finishing this assignment. \n",
    "> This includes peers, TAs, and links to online sources. \n",
    "\n",
    "Note that these citations will be taken into account during the grading and regrading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collaborators and sources:\n",
    "# Albert Einstein and Marie Curie\n",
    "# https://developers.google.com/edu/python/strings\n",
    "\n",
    "# your code here\n",
    "answer = 'my answer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c29820",
   "metadata": {},
   "source": [
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Do not change the number of cells!** Your submission notebook should have exactly one code cell per problem. \n",
    "* Do **not** remove the `# your code here` line and add you solution after that line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3389d87",
   "metadata": {},
   "source": [
    "### Some imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf79d2f",
   "metadata": {},
   "source": [
    "## 1. Web Scraping\n",
    "\n",
    "In this homework, we'll aquire data by scraping it from web pages. Fetching web pages, it turns out, is pretty easy; getting meaningful structured information out of them less so.\n",
    "\n",
    "### HTML and the Parsing Thereof\n",
    "\n",
    "Pages on the Web are written in HTML, in which text is (ideally) marked up into elements and their attributes:\n",
    "```\n",
    "<html>\n",
    "  <head>\n",
    "    <title>A web page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p id=\"author\">John Smith</p>\n",
    "    <p id=\"subject\">Data Science</p>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "In a perfect world, where all web pages are marked up semantically for our benefit, we would be able to extract data using rules like ‚Äúfind the `<p>` element whose id is author and return the text it contains.‚Äù In the actual world, HTML is not generally well-formed, let alone annotated. This means we‚Äôll need help making sense of it.\n",
    "\n",
    "To get data out of HTML, we will use the [BeautifulSoup library](http://www.crummy.com/software/BeautifulSoup/), which builds a tree out of the various elements on a web page and provides a simple interface for accessing them. We‚Äôll also be using the [requests library](http://docs.python-requests.org/en/latest/), which is a much nicer way of making HTTP requests than anything that‚Äôs built into Python.\n",
    "\n",
    "To use Beautiful Soup, we‚Äôll need to pass some HTML into the BeautifulSoup() function. In our examples, this will be the result of a call to requests.get:\n",
    "```\n",
    "html = requests.get(\"http://www.example.com\")\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "```\n",
    "after which we can get pretty far using a few simple methods.\n",
    "\n",
    "We‚Äôll typically work with Tag objects, which correspond to the tags representing the structure of an HTML page. For example, to find the first `<p>` tag (and extract its contents) you can use:\n",
    "```\n",
    "first_paragraph = soup.find('p').text\n",
    "```\n",
    "You can get multiple tags at once:\n",
    "```\n",
    "paragraphs = [p.text for p in soup.find_all('p')]\n",
    "```\n",
    "Frequently you‚Äôll want to find tags with a specific class:\n",
    "```\n",
    "paragraphs = [p.text for p in soup.find_all('p', class_=\"example_class\")]\n",
    "```\n",
    "will extract the text from each `<p>` tag with the class `example_class`. \n",
    "\n",
    "Just these handful of features will allow us to do quite a lot. If you end up needing to do more-complicated things (or if you‚Äôre just curious), check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#). Adapted from Joel Grus's [Data Science from Scratch](https://learning.oreilly.com/library/view/data-science-from/9781492041122/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26b85d",
   "metadata": {},
   "source": [
    "## 2. Get and Preprocess the Twitter Data\n",
    "\n",
    "Your next few tasks are to implement the following three functions that will pull and clean the news article summaries scraped from the [Associated Press](https://apnews.com/). The last task will be conducting a sentiment analysis on the data that has been pulled! As a fair warning, scraped HTML data can be pretty messy; your results may not be nearly as clean as the movie dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab693f9f",
   "metadata": {},
   "source": [
    "### Problem 2.1 \n",
    "\n",
    "**Do this!** Complete the following function that will take an arbitrary search term and page number and return a list of article summaries.\n",
    "\n",
    "1. Parse the HTML using Beautiful Soup.\n",
    "2. Find the first `div` tag with class `SearchResultsModule-results`.\n",
    "3. Find all the `div` elements nested within (2) with the class `PagePromo-description`. \n",
    "\n",
    "> **Hint**: Make sure it works before you continue. Look at the returned list, its length, and _some_ of its entries. **Best Practice**: Do not print out the entire list in the version of the notebook that you submit/deploy/share. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951e976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getResults(my_search_term, page=1):\n",
    "    \n",
    "    html = requests.get(f\"https://apnews.com/search?q={my_search_term}&p={page}\")\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    \n",
    "    results = ...\n",
    "    \n",
    "    # Don't worry about dates for now. We will use them in Problem 4.4\n",
    "    dates = [div.find('bsp-timestamp')['data-timestamp'] for div in soup.find_all(\"div\", class_=\"PagePromo-date\")]\n",
    "    \n",
    "    return results, dates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ee2b8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_search_term = \"data science\" # Replace this with your chosen search term! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff530bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, _ = getResults(my_search_term, 1)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a6a91",
   "metadata": {},
   "source": [
    "### Problem 2.2\n",
    "\n",
    "Now, we will need to process the article summaries. \n",
    "\n",
    "**Do this!** Complete the following function that processes _one_ article summary. \n",
    "* Use regex to remove special characters and punctuation.\n",
    "* Convert everything to all lowercase. \n",
    "* Split the result into a list of words. \n",
    "\n",
    "Return this list of words as `processedResult`.\n",
    "\n",
    "*Note: this question has hidden tests, or is graded on style of code and not just answer alone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b152b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preProcess(result):\n",
    " \n",
    "    processedResult = ...\n",
    "        \n",
    "    return processedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb215949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preProcess(results[np.random.randint(0,30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af2396",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580a6e3",
   "metadata": {},
   "source": [
    "### Problem 2.3\n",
    "\n",
    "Now, let's put it all together. Since we will only end up with 30 article summaries for each call of `getResults`,  we have to put the `getResults` and `preProcess` calls into a `while` loop. \n",
    "\n",
    "**Do this!** Complete the following function that will take in a search term then scrape and process the 100 article summaries which are most relevant to the given search term!\n",
    "\n",
    "* Create a `while` loop that runs as long as the length of `processedResults` is under `100`. In the while loop:\n",
    "    * Call the `getResults` function, passing in the `searchTerm` as well as a page number `page` to scrape.\n",
    "    * Create a loop going through each `result` in the list of returned results from `getResults`. In that loop:\n",
    "        * Run `preProcess` on each `result`, and append the returned list of words to `processedResults`. \n",
    "        * Make sure to break the inner loop once you have 100 processed results.\n",
    "    * Don't forget to increment `page` inside of the `while` loop so that we return a new, unseen chunk of article summaries with each call to `getResults`.\n",
    "\n",
    "*Note: this question has hidden tests, or is graded on style of code and not just answer alone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602bf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def searchTerm(my_search_term):\n",
    "    \n",
    "    processedResults = []\n",
    "    allDates = []\n",
    "    page = 1\n",
    "    \n",
    "    while ...:\n",
    "        results, dates = ... \n",
    "         \n",
    "        ...\n",
    "    \n",
    "        allDates += dates\n",
    "    \n",
    "    return processedResults, allDates[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab1877",
   "metadata": {},
   "source": [
    "Select your search term to test the functions.\n",
    "> **Hint**: Make sure this works before you continue. Look at the returned data, its type, length, and _some_ of its entries. **Best Practice**: Do not print out the entire data in the version of the notebook that you submit/deploy/share. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93483e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dates = searchTerm(my_search_term) # feel free to go change this above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a9c8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79762f",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3. Analyzing News Data\n",
    "\n",
    "Great! We now have all the data stored in our `data` variable. We can cycle through this data set and perform the same rule-based sentiment analysis that we saw previously in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd7069",
   "metadata": {},
   "source": [
    "### Problem 3.1\n",
    "\n",
    "**Write up!** Let's create a hypothesis about the sentiment of our news articles.\n",
    "* What would you guess the fraction of news articles with positive and negative emotions will be for your data/search term? Why?\n",
    "* Write this up (_before_ you perform the sentiment analysis) in the form of a **hypothesis (Q1)**.\n",
    "\n",
    "\n",
    "We will investigate how accurate this hypothesis was at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f92a3b",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fbc29",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 3.2\n",
    "\n",
    "**Do this!** Complete the following function that runs a rule-based sentiment analysis on _one_ given entry.\n",
    "* Set the score to zero, then loop through each word in the entry\n",
    "    * At each word, add one to the score if it is in `positive_words`, \n",
    "    * subtract one if it is in `negative_words`\n",
    "    * or do nothing if it is in neither!\n",
    "* Return `1` if the score is *not negative* and `-1` otherwise. \n",
    "\n",
    "\n",
    " > **Hint**: We will declare `positive_words` and `negative_words` as `global` variables, so don't bother about passing those in as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3bb28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyzeSentiment(entry):\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712dd96",
   "metadata": {},
   "source": [
    "Now, we can run all out tweets through this function and collect their sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "\n",
    "global negative_words\n",
    "global positive_words\n",
    "\n",
    "with open('utility/data/negative-words.txt') as f:\n",
    "    negative_words = [word.strip() for word in f.readlines() if word[0] not in [';', '\\n']]\n",
    "\n",
    "with open('utility/data/positive-words.txt') as f:\n",
    "    positive_words = [word.strip() for word in f.readlines() if word[0] not in [';', '\\n']]\n",
    "    \n",
    "for entry in data:\n",
    "    sentiments.append(analyzeSentiment(entry))\n",
    "sentiment_labels = np.array(sentiments)    \n",
    "    \n",
    "sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d4d7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87823d",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4. Visualizing the Results\n",
    "\n",
    "The final step is creating a few simple charts to look at the overall sentiment of the current search for news articles.\n",
    "\n",
    "### Problem 4.1\n",
    "\n",
    "**Do this!** Create a `bar` chart that visualizes the frequency of the positive and negative news articles in your dataset. Use appropriate axis labels, and include the search term (remember that you stored that in a variable earlier on) in your figure title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017a09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcdefaults() # Let's run a configuration to make prettier plots\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05239973",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 4.2\n",
    "\n",
    "**Write up!** Let's compare these results with our hypothesis **(Q1)**.\n",
    "* How accurate was your hypothesis?\n",
    "* What do you think could have caused your guess being very accurate or inaccurate? \n",
    "\n",
    "We will see another way of looking at this data to find more explanations in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d69e90",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b000a7",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Another Visualization: Wordclouds\n",
    "\n",
    "For a slightly more colorful view at the overall data, we can use a wordcloud module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(data2plot):\n",
    "    \n",
    "    overallWords = ' '\n",
    "\n",
    "    for entry in data2plot:\n",
    "        for word in entry:\n",
    "            overallWords += word + ' '\n",
    "\n",
    "    return overallWords\n",
    "\n",
    "wordcloud = WordCloud(width=600, height=430, max_words=50).generate(get_all_words(data))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb6794",
   "metadata": {},
   "source": [
    "Now, let's look at wordclouds based on the sentiment! To do this, we split up the data into one list of lists of all positive news articles and one for all negative news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = [i for indx,i in enumerate(data) if sentiments[indx] == 1]\n",
    "\n",
    "negative_data = [i for indx,i in enumerate(data) if sentiments[indx] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6da964",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 4.3\n",
    "\n",
    "**Do this!** Create one wordcloud for the postive articles and one for the negative articles, that _intuitivley_ visualizes your results. \n",
    "* Use a different `colormap` for each wordcloud (check out the availbale colormaps [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html)) **and/or** \n",
    "* play with the `background_color` (check out the available colors [here](https://matplotlib.org/stable/gallery/color/named_colors.html)).\n",
    "* Add appropriate titles to your subplots. \n",
    "\n",
    "> **Hint**: Follow the example above and use `get_all_words()`.\n",
    "\n",
    "> **[üêç Python Feature üêç]**: We can create figures with **multiple plots** using `plt.subplot`. The first number indicates the number of rows, the second input is the number of columns and the third is the plot you want to fill next. Like if you want to plot into the lower right corner of a figure with four plots in a 2x2 grid, then you would use `plt.subplot(224)` \n",
    "\n",
    "*Note: this question is graded on style/design choices of your visualization and not just on correctness alone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e704c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "plt.subplot(121)\n",
    "\n",
    "...\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dedc0d",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 4.4\n",
    "\n",
    "**Do this!** Create a line plot visualizing the sentiment on your chose topic over time. \n",
    "* First, sort the data in increasing order based on the dates the articles were published. These dates are stored in [Unix time](https://en.wikipedia.org/wiki/Unix_time). \n",
    "> **Hint**: Recall `hw0`, where we used a lambda function to retrieve values from a dictionary.\n",
    "* Next, group the sorted data into `k` bins. Choose `k` to be an appropriate value, so that 100 divides nicely by `k`. The first bin should contain the earliest `100/k` articles, while the `kth` bin should contain the most recent `100/k` articles. \n",
    "* Finally, for each bin, compute the fraction of positive news articles using the respective sentiment labels, as well as average date of publishing. Then, `fracs_pos` should contain the fraction of positive news articles for each bin, and `times` should contain the average date of publishing for each bin.\n",
    "> **Hint**: Because our dates are encoded in Unix time, we can compute the average date of publishing for a bin by averaging together the publishing dates for each article in the bin! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270e87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_data = zip(data, sentiment_labels, dates)\n",
    "\n",
    "k = ...\n",
    "\n",
    "fracs_pos = ...\n",
    "average_dates = ...\n",
    "...\n",
    "    \n",
    "plt.plot(range(k), fracs_pos, c='black')\n",
    "plt.xticks(range(k), [datetime.datetime.fromtimestamp(d/1000).strftime(\"%d %B %Y\") for d in average_dates], rotation = 90)\n",
    "plt.ylabel('Fraction of Positive Articles')\n",
    "plt.title('Fraction of Positive Articles over Time')\n",
    "plt.ylim(0, 1)\n",
    "plt.fill_between(range(k), 1, fracs_pos, where=fracs_pos, color='red', alpha=0.25)\n",
    "plt.fill_between(range(k), 0, fracs_pos, where=fracs_pos, color='green', alpha=0.25)\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c8d74",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5. Summarize your Findings\n",
    "\n",
    "### Problem 5.1\n",
    "\n",
    "**Write up!** The visualizations reveal a lot of information about your news data. Describe your basic findings by answering the following questions:\n",
    "\n",
    "- What was the overall sentiment? \n",
    "- What were some of the most/least frequent words that were used (larger = more common)? \n",
    "- Why do you think this is? \n",
    "- Do you believe this would be different during different weeks?\n",
    "- How do the words used and their frequency differ for positive versus negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a2e52",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296aac4c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Problem 5.2\n",
    "\n",
    "**Write up!** Elaborate on one specific thing or insight from your analysis that you find particularly interesing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09c452",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda85430",
   "metadata": {},
   "source": [
    "### Submission instructions\n",
    "* **Save your notebook**, then run the cell below to double-check your work. \n",
    "* Upload the `.ipynb` notebook directly to Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce159e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a0f0a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "otter": {
   "tests": {
    "q2a": {
     "name": "q2a",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> getResults('data science', 1)[0] != None\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(getResults('data science', 1)[0]) == list\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(getResults('data science', 1)[0][0]) == str\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> t = '\\n        This is an example article summary, with punctuation and whitespace.\\n    '\n>>> type(preProcess(t)) == list\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> t = '\\n        This is an example article summary, with punctuation and whitespace.\\n    '\n>>> type(preProcess(t)[0]) == str\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> t = '\\n        This is an example article summary, with punctuation and whitespace.\\n    '\n>>> len(preProcess(t)) == 10\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> t = '\\n        This is an example article summary, with punctuation and whitespace.\\n    '\n>>> np.all([p not in s for s in preProcess(t) for p in string.punctuation])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> t = '\\n        This is an example article summary, with punctuation and whitespace.\\n    '\n>>> np.all([re.search(r'\\s', s) is None for s in preProcess(t)])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(data[0]) == list\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(data[0][0]) == str\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(data) == 100\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> analyzeSentiment(['absence']) == -1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> analyzeSentiment(['this', 'is', 'a', 'bad', 'good', 'great', 'sentence']) == 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
